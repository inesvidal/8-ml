---
title: "Study on now well exercise is performed"
author: "Ines Vidal Casti√±eira"
output: html_document
---
```{r init, cache=TRUE, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
# To make code readable
library(knitr)
library(xtable)
library(markdown) 
library(rmarkdown) 
library(ggplot2) 
library(pander)
library(dplyr)
library(caret)
library(randomForest)
library(rpart)
library(rattle)
library(doMC)
registerDoMC(cores = 2)
library(YaleToolkit)
library(corrplot)
library(stats)
setwd("~/Coursera/8-ml/ml-project")
set.seed(12345)
#opts_chunk$set(echo = FALSE, cache = TRUE, fig.width=10, fig.height=6, fig.path='Figs/')#,warning=FALSE, message=FALSE, results='hide')
```
## Executive Summary

The HAR dataset contains measures of accelerometers worn by by six individuals while performing barbell lifts correctly and in five incorrect ways. Using this data we predict how well (in which manner) the exercise is done.
The study starts by reducing the dataset dimension, keeping only relevant variables. Then, to be able to assess out of sample error, the training dataset is divided in two, *train* and *test*. Using a sample of the train dataset the relationships between variables are studied and several models tested (random tree, random forest with and without PCA) and the one with the highest accuracy (and reasonable performance) in the *test* dataset is selected.

```{r, load-data, cache=TRUE, echo=FALSE}
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

filename_train <- "pml-training.csv"
filename_test <- "pml-testing.csv"
if (!file.exists(filename_train)) {
    print("INFO: csv file not found")
    ### Getting the dataset, and unzipping the files
    download.file(url_train, filename_train, method = "curl", cacheOK = FALSE)
    date_downloaded <- date()
    }
if (!file.exists(filename_test)) {
    print("INFO: csv file not found")
    ### Getting the dataset, and unzipping the files
    download.file(url_test, filename_test, method = "curl")
    date_downloaded <- date()
    }

#opening the data
training <- read.csv(filename_train)
testing <- read.csv(filename_test)
```

## Data cleaning

First we review sparsity, and remove rows without any informed value and columns with over 95% uninformed values (they coincide with columns that are only recorded when variable *new_window*="yes"). 
Finally we exclude columns *row.names*, *X*, *user_name*, *raw_timestamp_part_1*, *raw_timestamp_part_2*, *cvtd_timestamp*, *new_window* and *num_window*, that are considered irrelevant to the experiment.

```{r, clean-data, cache=TRUE, echo=FALSE}
# Prediction study design
# predicting the classe of exercise being performed (classe column)

#whatis(training_cl1)
clean <- function(x){
    # Unify blanks and #DIV/0! into NA
    # print("x0:")
    # print(dim(x))
    
    x[x == "" | x == "#DIV/0!"] <- NA
    
    # remove variables/columns with over a certain % of NAs
    # summary(colSums(is.na(x))/nrow(x)) # only a few collumns have a large number of NAs
    x <- x[colSums(is.na(x))/nrow(x) <.97]
    
    # remove observations/rows with over a certain % of NAs
    # summary(rowSums(is.na(x))/ncol(x))
    
    # check that all is fine
    # if(!anyNA(x)) 
    #    print("no NAs in x")
    return(x)
    }

training_cl <- clean(training)
testing_cl <- clean(testing)
#dim(training_cl) 
#dim(testing_cl)

# remove variables/columns considered irrelevant for study
testing_cl <- dplyr::select(testing_cl, -X, -user_name, -raw_timestamp_part_1, -raw_timestamp_part_2, -cvtd_timestamp, -new_window, -num_window)
training_cl <- dplyr::select(training_cl, -X, -user_name, -raw_timestamp_part_1, -raw_timestamp_part_2, -cvtd_timestamp, -new_window, -num_window)
```
That leaves us with a smaller dataset (`r dim(training_cl)[1]` observations and `r dim(training_cl)[2]` variables), in principle equally relevant in modelling terms, and easier to work with in terms of performance.

We decided to keep the test dataset for final validation, and subdivide the clean training set into a training set and a test set, so that we can assess out of sample error. The table below represents the different sets: 

```{r, prepare-data-sets, cache=TRUE, echo=FALSE}
# Creating working sets
inTrain <- createDataPartition(y=training_cl$classe, times = 1, p=0.998,list = FALSE)
train <- training_cl[inTrain,]
test <- training_cl[-inTrain,]
validate <- testing_cl

##############
# anyNA(train)
# anyNA(test)
# anyNA(validate)
table <- xtable(
    matrix(c(dim(training), dim(testing), dim(training_cl), dim(train), dim(test), dim(validate)), 
           ncol = 6, byrow = FALSE,
           dimnames = list(c("Observations", "Variables"), c("Original training dataset", "Original testing","Clean Training dataset", "Train set", "Test set","Validate set"))),
    caption = "Original datasets, and sets considered for the study",
    auto = TRUE)
#align = c("lll|lll"))
panderOptions('table.split.table', Inf)
pander(table)
```

##Data Exploration

```{r, explore-data-var, cache=TRUE, echo=FALSE, fig.width=8, fig.height=8}
# Study correlations between variables to select those most significant to predict classe
# dim(train)
# head(train)
zero_var <- sum(nearZeroVar(select(train, -classe), saveMetrics =TRUE, names = TRUE)$nzv)

```
Considering the *train set*, we'll start by checking the number of variables with zero variance (zero_var). 

Initial tests showed that execution times using the complete *train set* are very long, so we decided to proceed with a 1% sample (`dim(trainx)` observations), to assess model performance, and go ahead with final prediction only with the models that show good results,

Then we study how interrelated variables are (see figure below):

```{r, explore-data1, cache=TRUE, echo=FALSE, fig.width=8, fig.height=8}
cor <- cor(select(train, -classe))
corrplot(cor, method="color", type = "lower", tl.srt=45, tl.cex =.7, tl.col="black")

#<---------- REMOVE
#######################
# getting a slice of the train dataset to optimise execution time in first trials
inTrain2 <- createDataPartition(y=train$classe, times = 1, p=0.01,list = FALSE)
trainx <- train[inTrain2,]
#dim(trainx)
# anyNA(trainx)
```

Considering the number of dark cells (high correlation), a Principal Components analysis can help us identify the main variables to consider in the model, to reduce its complexity (and execution/training time). The negative values make a logaritmic transformation useless, so we proceed considering scaling and centering options. We combine the PCA preprocessing with a random forest model.

We tested the accuracy of a number of models considering the 1% sample of the *train set* with the following results:


```{r, explore_pca_rf, cache=TRUE, echo=FALSE, fig.width=8, fig.height=8}
x <- select(trainx, -classe)
y <- trainx$classe
z <- trainx
t <- test
system.time({
    # create preprocess object
    preProc <- preProcess(x, method="pca", thresh =.98) # calculate PCs for training data
    })
system.time({
    train_pca <- predict(preProc, x)
    })
#dim(preProc)
#dim(train_pca)
# run model on outcome and principle components
system.time({
    fit_pca_rf <- train(y ~ ., data = train_pca, method = "rf") # calculate PCs for test data
    })
system.time({
    test_pca_rf <- predict(preProc, select(t, -classe))
    })
# compare results 
system.time({
    acc_pca_rf <- confusionMatrix(t$classe, predict(fit_pca_rf, test_pca_rf))$overall[1]
    })
````

```{r, explore_pca_rf2, cache=TRUE, echo=FALSE, fig.width=8, fig.height=8}
print("fit_pca_rf")
system.time({
    fit_pca_rf2 <- train(y ~ ., data = z, method = "rf", preProcess = "pca", thresh =.98) # calculate PCs for test data    
    # compare results 
    acc_pca_rf2 <- confusionMatrix(t$classe, predict(fit_pca_rf2, t))$overall[1]
    })
````

```{r, explore_rpart, cache=TRUE, echo=FALSE, fig.width=8, fig.height=8}
print("fit_rpart")
train_p <- select(train, -classe)
system.time({
    fit_rpart <- train(classe ~ ., data = trainx, method = "rpart")
    # compare results 
    acc_rpart <- confusionMatrix(test$classe,predict(fit_rpart,select(test, -classe)))$overall[1]
    })
````

<------- pendiente revisar crosvarianza, se puede hacer dentro del random fores
```{r, explore_fit_rf, cache=TRUE, echo=FALSE, fig.width=8, fig.height=8}
print("fit_rf")
system.time({fit_rf <- randomForest(classe ~., data = z)})
cv <-rfcv(z, y)
with(cv, plot(n.var, error.cv, log="x", type="o", lwd=2))

print("fit_rf_caret con cv")
system.time({fit_rf_caret <- train(classe ~., data = z, trControl=trainControl(method="cv",number=10, repeats=1))})
print(fit_rf_caret$finalModel)

# predict for both cases
pred_rf <- predict(fit_rf, test)
pred_rf_caret <- predict(fit_rf_caret, test)
# obtain accuracy
acc_rf <- confusionMatrix(pred_rf, test$classe)$overall[1]
acc_pca_rf2 <- confusionMatrix(pred_rf, test$classe)$overall[1]
acc_pca_caret <- confusionMatrix(pred_rf_caret, test$classe)$overall[1]

final_acc <- rbind(acc_pca_rf, acc_pca_rf2, acc_rpart, acc_rf, acc_rf, acc_pca_caret)

```



```{r, model-selection, cache=TRUE, echo=FALSE, fig.width=8, fig.height=8}
t <- xtable(final_acc,
#     matrix(final_acc, 
#            ncol = 2, byrow = FALSE,
#            dimnames = list(NULL, NULL),
    caption = "Models accuracy comparison using 1% sample of 'train set'")
panderOptions('table.split.table', Inf)
pander(t)
```
```{r, validate, cache=TRUE, echo=FALSE, fig.width=8, fig.height=8}
# assuming fit_rf is the best
system.time({fit_rf_total <- randomForest(train$classe ~., data = select(train, -classe))})
#study of cross validation <- very time consuming
cv <-rfcv(train, train$classe)
with(cv, plot(n.var, error.cv, log="x", type="o", lwd=2))
# study of out of sample error
pred_rf_total <- predict(fit_rf_total, test)
acc_rf_total <- confusionMatrix(pred_rf_total, test$classe)$overall[1]

# predict results on validation set to submit assignment
result <- predict(fit_rf_total, validate)

```


##We will consider models that with independence of the internal treatment that some models already consider (e.g. random forest)


## Analysis

## Conclusions

\newpage
## References

In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.
